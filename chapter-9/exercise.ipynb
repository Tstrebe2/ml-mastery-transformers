{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tstrebe2/ml-mastery-transformers/blob/main/chapter-9/exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9.2 The SimpleRNN Network\n",
        "In this section, you’ll write the basic code to generate the dataset and use a SimpleRNN network to predict the next number of the Fibonacci sequence. Let’s first write the import section:"
      ],
      "metadata": {
        "id": "YisIBLC6Ze2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, SimpleRNN\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CycHaCf9ZfSq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the Dataset**  \n",
        "The following function generates a sequence of n Fibonacci numbers (not counting the starting two values). If ```scale_data``` is set to ```True```, then it would also use the ```MinMaxScaler``` from scikitlearn to scale the values between 0 and 1. Let’s see its output for n = 10.\n"
      ],
      "metadata": {
        "id": "IJfjtyj1cGxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fib_seq(n, scale_data=True):\n",
        "  # Get the Fibonacci sequence\n",
        "  seq = np.zeros(n)\n",
        "  fib_n1 = 0.0\n",
        "  fib_n = 1.0\n",
        "  for i in range(n):\n",
        "    seq[i] = fib_n1 + fib_n\n",
        "    fib_n1 = fib_n\n",
        "    fib_n = seq[i]\n",
        "    scaler = []\n",
        "  if scale_data:\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    seq = np.reshape(seq, (n, 1))\n",
        "    seq = scaler.fit_transform(seq).flatten()\n",
        "  return seq, scaler\n",
        "\n",
        "fib_seq, _ = get_fib_seq(10, False)\n",
        "print(fib_seq)"
      ],
      "metadata": {
        "id": "-rStKZ8IcGD8",
        "outputId": "6c4ce6fe-80a3-4d66-f776-717986f30913",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1.  2.  3.  5.  8. 13. 21. 34. 55. 89.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need a function ```get_fib_XY()``` that reformats the sequence into input features and target values to be used by the Keras input layer. When given ```time_steps``` as a parameter, ```get_fib_XY()``` constructs each row of the dataset with ```time_steps``` number of columns. This function not only constructs the training set and test set from the Fibonacci sequence but also shuffles the training examples and reshapes them to the required ```TensorFlow``` format, i.e., (total_samples, time_steps, features). Also, the function returns the scaler object that scales the values if scale_data is set to True. Let’s generate a small training set to see what it looks like. We have set ```time_steps=3``` and ```total_fib_numbers=12```, with approximately 30% of the examples going toward the test points. Note the training and test examples have been shuffled by the ```permutation()``` function."
      ],
      "metadata": {
        "id": "ypmyy-FLc9fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
        "  dat, scaler = get_fib_seq(total_fib_numbers, scale_data)\n",
        "  Y_ind = np.arange(time_steps, len(dat), 1)\n",
        "  Y = dat[Y_ind]\n",
        "  rows_x = len(Y)\n",
        "  X = dat[0:rows_x]\n",
        "  for i in range(time_steps-1):\n",
        "    temp = dat[i+1:rows_x+i+1]\n",
        "    X = np.column_stack((X, temp))\n",
        "  # random permutation with fixed seed\n",
        "  rand = np.random.RandomState(seed=13)\n",
        "  idx = rand.permutation(rows_x)\n",
        "  split = int(train_percent*rows_x)\n",
        "  train_ind = idx[0:split]\n",
        "  test_ind = idx[split:]\n",
        "  trainX = X[train_ind]\n",
        "  trainY = Y[train_ind]\n",
        "  testX = X[test_ind]\n",
        "  testY = Y[test_ind]\n",
        "  trainX = np.reshape(trainX, (len(trainX), time_steps, 1))\n",
        "  testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
        "  return trainX, trainY, testX, testY, scaler\n",
        "  \n",
        "trainX, trainY, testX, testY, scaler = get_fib_XY(12, 3, 0.7, False)\n",
        "print('trainX = ', trainX)\n",
        "print('trainY = ', trainY)"
      ],
      "metadata": {
        "id": "aFx00Fn_dxVc",
        "outputId": "5f0b5d0a-1540-41c4-c805-1ad797e7c1f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX =  [[[ 8.]\n",
            "  [13.]\n",
            "  [21.]]\n",
            "\n",
            " [[ 5.]\n",
            "  [ 8.]\n",
            "  [13.]]\n",
            "\n",
            " [[ 2.]\n",
            "  [ 3.]\n",
            "  [ 5.]]\n",
            "\n",
            " [[13.]\n",
            "  [21.]\n",
            "  [34.]]\n",
            "\n",
            " [[21.]\n",
            "  [34.]\n",
            "  [55.]]\n",
            "\n",
            " [[34.]\n",
            "  [55.]\n",
            "  [89.]]]\n",
            "trainY =  [ 34.  21.   8.  55.  89. 144.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up the Network**  \n",
        "Now let’s set up a small network with two layers. The first one is the ```SimpleRNN``` layer, and the second one is the ```Dense``` layer. Below is a summary of the model."
      ],
      "metadata": {
        "id": "qu4AbuUjfVSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up parameters\n",
        "time_steps = 20\n",
        "hidden_units = 2\n",
        "epochs = 30\n",
        "# Create a traditional RNN network\n",
        "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "  model = Sequential()\n",
        "  model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
        "  model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1,\n",
        "input_shape=(time_steps,1), activation=['tanh', 'tanh'])\n",
        "model_RNN.summary()"
      ],
      "metadata": {
        "id": "749CYdoJfZnD",
        "outputId": "6f9bd225-abbc-47b3-fdba-8e333f0848cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 2)                 8         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11\n",
            "Trainable params: 11\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Network and Evaluate**  \n",
        "The next step is to add code that generates a dataset, trains the network, and evaluates it. This time around, we’ll scale the data between 0 and 1. We don’t need to pass the scale_data parameter as its default value is True."
      ],
      "metadata": {
        "id": "qMnlvzKzhqg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the dataset\n",
        "trainX, trainY, testX, testY, scaler = get_fib_XY(1200, time_steps, 0.7)\n",
        "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
        "# Evalute model\n",
        "train_mse = model_RNN.evaluate(trainX, trainY)\n",
        "test_mse = model_RNN.evaluate(testX, testY)\n",
        "# Print error\n",
        "print(\"Train set MSE = \", train_mse)\n",
        "print(\"Test set MSE = \", test_mse)"
      ],
      "metadata": {
        "id": "v-Kr6__ohqKd",
        "outputId": "68ee593f-5cbe-4f57-eafa-9206f22434d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "826/826 - 4s - loss: 5.4785e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 2/30\n",
            "826/826 - 2s - loss: 4.6600e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 3/30\n",
            "826/826 - 2s - loss: 3.7265e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 4/30\n",
            "826/826 - 3s - loss: 3.1117e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 5/30\n",
            "826/826 - 2s - loss: 2.5758e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 6/30\n",
            "826/826 - 3s - loss: 2.0816e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 7/30\n",
            "826/826 - 3s - loss: 1.6574e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 8/30\n",
            "826/826 - 3s - loss: 1.3307e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 9/30\n",
            "826/826 - 3s - loss: 1.0586e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 10/30\n",
            "826/826 - 2s - loss: 8.4531e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 11/30\n",
            "826/826 - 3s - loss: 7.3276e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 12/30\n",
            "826/826 - 2s - loss: 6.9532e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 13/30\n",
            "826/826 - 2s - loss: 6.7584e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 14/30\n",
            "826/826 - 2s - loss: 6.5054e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 15/30\n",
            "826/826 - 2s - loss: 6.2539e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 16/30\n",
            "826/826 - 2s - loss: 6.4303e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 17/30\n",
            "826/826 - 2s - loss: 6.3794e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 18/30\n",
            "826/826 - 2s - loss: 6.1903e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 19/30\n",
            "826/826 - 2s - loss: 6.1050e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 20/30\n",
            "826/826 - 3s - loss: 6.3704e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 21/30\n",
            "826/826 - 2s - loss: 5.9881e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 22/30\n",
            "826/826 - 2s - loss: 6.1581e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 23/30\n",
            "826/826 - 2s - loss: 6.0921e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 24/30\n",
            "826/826 - 2s - loss: 6.0326e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 25/30\n",
            "826/826 - 2s - loss: 5.9937e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 26/30\n",
            "826/826 - 2s - loss: 5.9458e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 27/30\n",
            "826/826 - 2s - loss: 6.0404e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 28/30\n",
            "826/826 - 2s - loss: 5.8196e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 29/30\n",
            "826/826 - 2s - loss: 5.8992e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 30/30\n",
            "826/826 - 2s - loss: 5.6252e-05 - 2s/epoch - 3ms/step\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 4.6568e-05\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 1.7453e-05\n",
            "Train set MSE =  4.656817691284232e-05\n",
            "Test set MSE =  1.7453086911700666e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9.3 Adding a Custom Attention Layer to the Network  \n",
        "In Keras, it is easy to create a custom layer that implements attention by subclassing the Layer class. The Keras guide lists clear steps for creating a new layer via subclassing. You’ll use those guidelines here. All the weights and biases corresponding to a single layer are\n",
        "encapsulated by this class. You need to write the ```__init__``` method as well as override the following methods:  \n",
        "⊲ ```build()```: The Keras guide recommends adding weights in this method once the size of the inputs is known. This method “lazily” creates weights. The built-in function ```add_weight()``` can be used to add the weights and biases of the attention layer.  \n",
        "⊲ ```call()```: The ```call()``` method implements the mapping of inputs to outputs. It should implement the forward pass during training.\n",
        "  \n",
        "**The Call Method for the Attention Layer**  \n",
        "The call method of the attention layer has to compute the alignment scores, weights, and context. You can go through the details of these parameters in Chapter 8. You’ll implement the Bahdanau attention in your ```call()``` method. The good thing about inheriting a layer from the Keras Layer class and adding the weights via the ```add_weights()``` method is that weights are automatically tuned. Keras does an equivalent of “reverse engineering” of the operations/computations of the ```call()``` method and calculates the gradients during training.  \n",
        "It is important to specify ```trainable=True``` when adding the weights. You can also add a ```train_step()``` method to your custom layer and specify your own method for weight training if needed.  \n",
        "The code below implements the custom attention layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "JPVW2aoJiscq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class attention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1), \n",
        "                             initializer='random_normal', trainable=True)\n",
        "    self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1), \n",
        "                             initializer='zeros', trainable=True)\n",
        "    super().build(input_shape)\n",
        "\n",
        "  def call(self, x):\n",
        "    # Alignment scores. Pass them through tanh function\n",
        "    e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "    # Remove dimension of size 1\n",
        "    e = K.squeeze(e, axis=-1)\n",
        "    # Compute the weights\n",
        "    alpha = K.softmax(e)\n",
        "    # Reshape to tensorFlow format\n",
        "    alpha = K.expand_dims(alpha, axis=-1)\n",
        "    # Compute the context vector\n",
        "    context = x * alpha\n",
        "    context = K.sum(context, axis=1)\n",
        "    return context"
      ],
      "metadata": {
        "id": "am7d3WIQi7P-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN with Attention Layer**  \n",
        "Let’s now add an attention layer to the RNN network you created earlier.\n",
        "The attention layer expects a sequence as input. To use it after the ```SimpleRNN``` layer, the latter should return a sequence. The function ```create_RNN_with_attention()``` now specifies an RNN layer, an ```attention``` layer, and a ```Dense``` layer in the network. Make sure to set return_sequences=True when specifying the SimpleRNN. This will return the output of the hidden units for all the previous time steps, i.e., as a sequence.  \n",
        "\n",
        "Let’s look at a summary of the model with attention."
      ],
      "metadata": {
        "id": "4y7EJLyhppOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
        "  x = Input(shape=input_shape)\n",
        "  RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
        "  attention_layer = attention()(RNN_layer)\n",
        "  outputs = Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
        "  model = Model(x, outputs)\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1,\n",
        "                                            input_shape=(time_steps, 1), activation='tanh')\n",
        "model_attention.summary()"
      ],
      "metadata": {
        "id": "mVaFgrejsCJu",
        "outputId": "cb39c4d4-4122-4519-a53e-129dea96f766",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
            "                                                                 \n",
            " attention (attention)       (None, 2)                 22        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33\n",
            "Trainable params: 33\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Evaluate the Deep Learning Network with Attention**  \n",
        "It’s time to train and test your model and see how it performs in predicting the next Fibonacci number of a sequence.\n"
      ],
      "metadata": {
        "id": "nsg--5BTtpwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
        "# Evalute model\n",
        "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
        "test_mse_attn = model_attention.evaluate(testX, testY)\n",
        "# Print error\n",
        "print(\"Train set MSE with attention = \", train_mse_attn)\n",
        "print(\"Test set MSE with attention = \", test_mse_attn)"
      ],
      "metadata": {
        "id": "Ph49J6tSt7-0",
        "outputId": "8b52eb86-5d52-49fa-a544-402fae6ea9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "826/826 - 3s - loss: 0.0014 - 3s/epoch - 4ms/step\n",
            "Epoch 2/30\n",
            "826/826 - 2s - loss: 0.0014 - 2s/epoch - 3ms/step\n",
            "Epoch 3/30\n",
            "826/826 - 2s - loss: 0.0014 - 2s/epoch - 3ms/step\n",
            "Epoch 4/30\n",
            "826/826 - 3s - loss: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 5/30\n",
            "826/826 - 3s - loss: 0.0014 - 3s/epoch - 3ms/step\n",
            "Epoch 6/30\n",
            "826/826 - 3s - loss: 0.0013 - 3s/epoch - 3ms/step\n",
            "Epoch 7/30\n",
            "826/826 - 2s - loss: 0.0013 - 2s/epoch - 3ms/step\n",
            "Epoch 8/30\n",
            "826/826 - 2s - loss: 0.0013 - 2s/epoch - 3ms/step\n",
            "Epoch 9/30\n",
            "826/826 - 2s - loss: 0.0012 - 2s/epoch - 3ms/step\n",
            "Epoch 10/30\n",
            "826/826 - 2s - loss: 0.0012 - 2s/epoch - 3ms/step\n",
            "Epoch 11/30\n",
            "826/826 - 2s - loss: 0.0012 - 2s/epoch - 3ms/step\n",
            "Epoch 12/30\n",
            "826/826 - 3s - loss: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 13/30\n",
            "826/826 - 3s - loss: 0.0010 - 3s/epoch - 3ms/step\n",
            "Epoch 14/30\n",
            "826/826 - 2s - loss: 9.6853e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 15/30\n",
            "826/826 - 3s - loss: 9.0350e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/30\n",
            "826/826 - 3s - loss: 8.3990e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/30\n",
            "826/826 - 2s - loss: 7.6750e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 18/30\n",
            "826/826 - 2s - loss: 6.8811e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 19/30\n",
            "826/826 - 3s - loss: 6.2671e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/30\n",
            "826/826 - 2s - loss: 5.6308e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 21/30\n",
            "826/826 - 2s - loss: 4.9619e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 22/30\n",
            "826/826 - 3s - loss: 4.3397e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 23/30\n",
            "826/826 - 2s - loss: 3.8370e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 24/30\n",
            "826/826 - 3s - loss: 3.4304e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/30\n",
            "826/826 - 2s - loss: 2.9912e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 26/30\n",
            "826/826 - 3s - loss: 2.6840e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 27/30\n",
            "826/826 - 3s - loss: 2.3028e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 28/30\n",
            "826/826 - 2s - loss: 2.1195e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 29/30\n",
            "826/826 - 2s - loss: 1.9091e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 30/30\n",
            "826/826 - 2s - loss: 1.7702e-04 - 2s/epoch - 3ms/step\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 1.4093e-04\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7266e-05\n",
            "Train set MSE with attention =  0.00014093257777858526\n",
            "Test set MSE with attention =  1.726628397591412e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that even for this simple example, the mean square error on the test set is lower with the attention layer. You can achieve better results with hyper-parameter tuning and model selection. Try this out on more complex problems and by adding more layers to the network. You can also use the scaler object to scale the numbers back to their original values.  \n",
        "\n",
        "You can take this example one step further by using LSTM instead of SimpleRNN, or you can build a network via convolution and pooling layers. You can also change this to an encoder-decoder network if you like.\n"
      ],
      "metadata": {
        "id": "xSFlediauzlh"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}